{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "\n",
    "torch.manual_seed(420)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postavka problema: Opstanak novih igrača u NBA ligi\n",
    "\n",
    "Jedan od čestih problema menadžera timova u NBA ligi je procena da li je igrač koji je upravo došao u ligu ima potencijala da u njoj igra na duže staze. \n",
    "Menadžeri se pored mišljenja trenera i ostalih igrača vrlo često oslanjaju na analizu statistike igrača.\n",
    "U ovom radu predlažemo model koji će na osnovu statistike prve sezone igrača u NBA generisati predlog da li je vredno zadržati igrača u timu.\n",
    "\n",
    "Treniranje i evaluacija vršena je na datasetu [5 Year Survival of NBA Rookies from 1980-2015](https://www.kaggle.com/datasets/mamadoudiallo/5-year-survival-of-nba-rookies-from-19802015) u kojem je računata isplativost zadržavanja igrača u timu ako on i dalje ima ugovor nakon 5 godina po dolasku u ligu.\n",
    "Dataset sadrži sve draftovane igrače u periodu od 1980. do 2015, njihovu statistiku u prvoj sezoni, kao i to da li su ostali u ligi nakon 5 godina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Istraživanje podataka i predprocesiranje\n",
    "\n",
    "U nastavku su podaci učitani u *dataframe* i urađeno je osnovno predprocesiranje kako bi se smanjila dimenzionalnost problema.\n",
    "Svaka od odluka pri filtriranju baze biće objašnjena neposredno pre ili posle odgovarajućeg bloka koda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .csv file into pandas dataframe\n",
    "df = pd.read_csv(\"data/rookie_df.csv\", low_memory=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Odabir relevantnih atributa\n",
    "\n",
    "U datasetu postoji 22 atributa za svakiog igraca, medju kojima postoje neki koji su očigledno korelisani.\n",
    "Uzmimo za primer slobodna bacanja: za svakog igrača je vođena statistika koliko bacanja je igrač šutira (`FGA`), koliko ih je pogodio (`FGM`), kao i procenat uspešnih bacanja (`FG%`).\n",
    "Očigledno je da je procenat moguće nedvosmisleno iračunati iz prva dva podatka, tako da je on otklonjen.\n",
    "Odluka da se zadže dva broja umesto procenta doneta je kako bi model mogao da uračuna situacije u kojima je igrač imao mali broj pokušaja i samim time procenat ne prenosi adekvatno njegovu stvarnu efikasnost.\n",
    "\n",
    "U nastavku je zadržano 16 atributa i otklonjeni su svi igrači sa nedefinisanim poljima.\n",
    "\n",
    "| Atribut | Opis                                        |\n",
    "|---------|---------------------------------------------|\n",
    "| GP      | Broj odigranih utakmica                     |\n",
    "| MIN     | Prosečan broj minuta u igri                 |\n",
    "| PTS     | Prosečan broj postignutih poena             |\n",
    "| FGM     | Prosečan broj postignutih šuteva iz igre    |\n",
    "| FGA     | Prosečan broj pokušanih šuteva iz igre      |\n",
    "| 3P Made | Prosečan broj postignutih šuteva za 3 poena |\n",
    "| 3PA     | Prosečan broj pokušanih šuteva za 3 poena   |\n",
    "| FTM     | Prosečan broj pogođenih slobodnih bacanja   |\n",
    "| FTA     | Prosečan broj pokušanih slobodnih bacanja   |\n",
    "| OREB    | Prosečan broj skokova u napadu              |\n",
    "| DREB    | Prosečan broj skokova u odbrani             |\n",
    "| AST     | Prosečan broj asistencija                   |\n",
    "| STL     | Prosečan broj ukradenih lopti               |\n",
    "| BLK     | Prosečan broj blokiranih šuteva             |\n",
    "| TOV     | Prosečan broj izgubljenih lopti             |\n",
    "| EFF     | Prosečan skor efikasnosti igrača            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose valuable features\n",
    "clean_df = df[['GP', 'MIN', 'PTS', 'FGM', 'FGA',\n",
    "       '3P Made', '3PA', 'FTM', 'FTA', 'OREB', 'DREB',\n",
    "       'AST', 'STL', 'BLK', 'TOV', 'EFF', 'target']].copy()\n",
    "clean_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe features\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of True datapoints\n",
    "value_cnts = clean_df[\"target\"].value_counts()\n",
    "print(f\"Dataset sadrži {len(clean_df)} igrača od kojih je {value_cnts[1] / len(clean_df) * 100:0.2f}% ostalo u ligi nakon 5 godina.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podela na trening i test\n",
    "\n",
    "Dataset je podeljen na trening i test skup, od čega trening set sadrži 70% podataka.\n",
    "Korištena je fukncija `train_test_split` koja nasumično promeni mesta podataka u tabeli pre nego što uradi podelu na dva seta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Do train/test split\n",
    "train_df, valid_df = train_test_split(clean_df, train_size=.7, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizacija dataseta\n",
    "\n",
    "Pošto se svi atributi u datasetu ne nalaze u istim opsezima, izvršena je normalizacija podataka.\n",
    "Za ovu svrhu upotrebljena je `StandardScaler` klasa, ima mogućnost fitovanja parametara normalizacije na jednom skupu, i odvojenu primenu iste.\n",
    "Parametri normalizacije su fitovani na trening setu, a onda primenjeni i na treningu i na testu.\n",
    "Na ovaj način nije došlo do curenja informacija iz testa u trening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "input_columns = clean_df.columns[:-1]\n",
    "\n",
    "# Fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[input_columns].values)\n",
    "\n",
    "# Normalize train data\n",
    "norm_train_np = scaler.transform(train_df[input_columns].values)\n",
    "norm_train_df = pd.DataFrame(norm_train_np, columns=input_columns)\n",
    "norm_train_df[\"target\"] = train_df[\"target\"].values\n",
    "\n",
    "# Normalize validation data\n",
    "norm_valid_np = scaler.transform(valid_df[input_columns])\n",
    "norm_valid_df = pd.DataFrame(norm_valid_np, columns=input_columns)\n",
    "norm_valid_df[\"target\"] = valid_df[\"target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovime je završeno predprocesiranje podataka, i oni su spremni za treniranje modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definisanje modela\n",
    "\n",
    "Radi predikcije dugovečnosti igrača na osnovu statistike u prvoj godini implementirana je poptpuno povezana neuronska mreža.\n",
    "Mrežu je moguće inicilaizovati sa proizvoljnim brojem skrivenih slojeva, a postoji tačno jedan izlazni neuron.\n",
    "Izlaz modela predstavlja njegovo poverenje da će igrač opstati u ligi, u rasponu [0, 1].\n",
    "\n",
    "Svi delovi generisanja modela, treninga i evaluacije dekomponovani su u odgovarajuće funkcije i klase radi jednostavnijeg eksperimentisanja kasnije."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasa za učitavanje podataka\n",
    "\n",
    "Tokom treninga je poželjno da pri tokom svake epohe treninga, podaci budu prosleđeni modelu u različitim redosledima.\n",
    "`pytorch` obezbeđuje ovu funkcionalnost, kao i jednostavno čitanje podataka po *batch*-evima iteracijom kroz klasu `Dataloader`.\n",
    "Ova klasa pri svojoj inicijalizaciji zahteva objekat koji nasleđuje klasu `Dataset`.\n",
    "\n",
    "`Dataset` je klasa koja sadrži sve podatke treninga/testa i potrebno je u njoj implementirati metod `__getitem__` koji vraća ulazne podatke kao i očekivanu vrednost za model, na osnovu indeksa u bazi.\n",
    "U nastavku je implementirana klasa `NBA_Dataset`, koja obezbeđuje ovu funkcionalnost za odgovarajući dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NBA_Dataset(Dataset):\n",
    "    \"\"\"Class for holding data NBA rookie statistics, and if they remained in the league after 5 year .\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Initialize torch Dataset based on pandas Dataframe.\"\"\"\n",
    "        super(NBA_Dataset, self).__init__()\n",
    "\n",
    "        # Take all except the last column as inputs\n",
    "        self.inputs = df.values[:, :-1].astype(float)\n",
    "\n",
    "        # Cast Targets column to numpy\n",
    "        self.targets = df[\"target\"].values.astype(int)\n",
    "\n",
    "        # Save dataframe as part of the class\n",
    "        self.df = df\n",
    "\n",
    "        # Define dataset length\n",
    "        self.len = len(df)\n",
    "\n",
    "    def __getitem__(self, index) -> dict:\n",
    "        \"\"\"Return dict with information of datapoint at `index`\"\"\"\n",
    "        # Get input\n",
    "        input_row = torch.Tensor(self.inputs[index, :])\n",
    "\n",
    "        # Get output\n",
    "        target = torch.Tensor([self.targets[index]])\n",
    "\n",
    "        return {\n",
    "            \"Inputs\": input_row,\n",
    "            \"Targets\": target,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get dataset length.\"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon inicijalizacije `Dataset` klase, ona je *wrap*-ovana u `Dataloader` klasu.\n",
    "U okviru ove klase su definisane veličina *batch*-a, broj radnih jedinica, kao i opcija da li redosled podataka treba randomizovati pred svaku epohu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NBA_Dataset(norm_train_df)\n",
    "\n",
    "train_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "\n",
    "training_loader = DataLoader(train_dataset, **train_parameters)\n",
    "\n",
    "# Validation dataset\n",
    "valid_dataset = NBA_Dataset(norm_valid_df)\n",
    "\n",
    "valid_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "\n",
    "validation_loader = DataLoader(valid_dataset, **valid_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arhitektura modela\n",
    "\n",
    "Model je implementiran u klasi `NBA_Survival_Predictor`.\n",
    "Mrežu je moguće inicijalizovati sa proizvoljnim brojem skrivenih slojeva, čije se veličine prosleđuju konstruktoru u vidu liste.\n",
    "Model je takođe moguće inicijalizovati sa proizvoljnom aktivacionom funkciojom, a ako ona nije prosleđena, aktivaciona fukcija će biti hiperbolički tangens.\n",
    "\n",
    "U okviru `forward` funkcije implementiran je prolazak kroz model.\n",
    "Podaci se provlače kroz sve inicijalizovane objekte u listi slojeva.\n",
    "Objekti su naizmenično potpuno povezani slojevi i aktivacione funkcije, a umesto poslednje aktivacione funkcije primenjen sigmoid.\n",
    "Sigmoidna funkcija je upotrebljena kao standardna aktivaciona funkcija izlaznog sloja, prilikom klasifikacionih problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NBA_Survival_Predictor(nn.Module):\n",
    "    \"\"\"Class for estimating chance of NBA rookie surviving in the league.\"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes: list, activation_function: nn.Module = None) -> None:\n",
    "        \"\"\"Initialize fully-connected model based on input parameters.\"\"\"\n",
    "        super(NBA_Survival_Predictor, self).__init__()\n",
    "\n",
    "        # Check if input and output layers are passed\n",
    "        assert len(layer_sizes) >= 2\n",
    "\n",
    "        # Check if activation function is not passed\n",
    "        if activation_function is None:\n",
    "            activation_function = nn.ReLU()\n",
    "\n",
    "        # Define activation function\n",
    "        self.activation = activation_function\n",
    "\n",
    "        # Initialize list of layers\n",
    "        layer_list = []\n",
    "\n",
    "        # Initialize each layer and activation function\n",
    "        for in_size, out_size in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            layer_list.append(nn.Linear(in_size, out_size))\n",
    "            layer_list.append(nn.Tanh())\n",
    "\n",
    "        # Cast layer list to nn.Module\n",
    "        self.fc_layers = nn.ModuleList(layer_list)\n",
    "\n",
    "    def forward(self, X) -> torch.Tensor:\n",
    "        \"\"\"Define network behavior when called on data.\"\"\"\n",
    "        # Pass data trough fully-connected layers\n",
    "        # Skip last activation function\n",
    "        for layer in self.fc_layers[:-1]:\n",
    "            X = layer(X)\n",
    "        \n",
    "        # Calculate output probability as sigmoid of output\n",
    "        output_probability = torch.sigmoid(X)\n",
    "\n",
    "        return output_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definisanje treninga i evaluacije\n",
    "\n",
    "U nastavku se nalazi implementacija funkcije za treniranje i evaluaciju modela.\n",
    "Funkciji od ulaznih parametara očekuje model, optimizator i DICTIONARY trenaznih objekata.\n",
    "DICTIONARY u sebi sadrži 4 polja, *loss* funkciju, *dataloader* za trening i validaciju, kao i broj epoha za trening.\n",
    "\n",
    "U okviru funkcije, model je treniran po *batch*-evima, a zatim evaluiran.\n",
    "Na kraju svake epohe, sačuvani su vrednost *loss*-a i tačnost modela.\n",
    "Tačnost je odabrana kao glavna metrika performanse modela zato što je *dataset* balansiran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_THR = .5\n",
    "def train_network(model: NBA_Survival_Predictor, optimizer: torch.optim.Optimizer, training_objects: dict) -> tuple:\n",
    "    \"\"\"Function trains and evaluates defined model using given optimizer on passed training objects.\n",
    "\n",
    "    Returned values of the function are trained model, and a dictionary with training and evaluation\n",
    "    metrics.\n",
    "\n",
    "    Args:\n",
    "        model (NBA_Survival_Predictor): Model for training.\n",
    "        optimizer (torch.optim.Optimizer): Model optimizer.\n",
    "        training_objects (dict): Dictionary containing loss function, number of epochs, and\n",
    "                                 training and validation loaders.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Passed model after training, and dictionary containing training and validation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack dictionaries\n",
    "    loss_function = training_objects[\"Loss Function\"]\n",
    "    training_loader = training_objects[\"Train Dataloader\"]\n",
    "    validation_loader = training_objects[\"Valid Dataloader\"]\n",
    "    num_of_epochs = training_objects[\"Epochs\"]\n",
    "\n",
    "    # Initialize lists for logging training and validation metrics\n",
    "    training_loss = []\n",
    "    training_acc = []\n",
    "    validation_loss = []\n",
    "    validation_acc = []\n",
    "\n",
    "    # Train model for set number of epochs\n",
    "    for _ in range(num_of_epochs):\n",
    "\n",
    "        # Reset training metrics\n",
    "        train_epoch_loss = 0.0\n",
    "        train_epoch_acc = 0.0\n",
    "\n",
    "        # Set model in train mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop over training set in batches\n",
    "        for batch in training_loader:\n",
    "            \n",
    "            # Unpack batch\n",
    "            X = batch[\"Inputs\"]\n",
    "            Y = batch[\"Targets\"]\n",
    "\n",
    "            # Reset optimizer gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Run a forward pass\n",
    "            probabilities = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(probabilities, Y)\n",
    "            train_epoch_loss += loss.item()\n",
    "\n",
    "            # Update model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate predictions\n",
    "            predictions = probabilities >= CLASS_THR\n",
    "\n",
    "            # Log true predictions\n",
    "            train_epoch_acc += sum(predictions == Y)\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        train_epoch_loss /= len(training_loader)\n",
    "        train_epoch_acc /= len(training_loader.dataset)\n",
    "\n",
    "        # Log epoch metrics\n",
    "        training_loss.append(train_epoch_loss)\n",
    "        training_acc.append(train_epoch_acc)\n",
    "\n",
    "        # Reset validation metrics\n",
    "        valid_epoch_loss = 0.0\n",
    "        valid_epoch_acc = 0.0\n",
    "\n",
    "        # Set model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Loop over validation set in batches\n",
    "        for batch in validation_loader:\n",
    "            \n",
    "            # Unpack batch\n",
    "            X = batch[\"Inputs\"]\n",
    "            Y = batch[\"Targets\"]\n",
    "\n",
    "            # Run forward pass\n",
    "            probabilities = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_function(probabilities, Y)\n",
    "            valid_epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions\n",
    "            predictions = probabilities >= CLASS_THR\n",
    "\n",
    "            # Log true predictions\n",
    "            valid_epoch_acc += sum(predictions == Y)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        valid_epoch_loss /= len(validation_loader)\n",
    "        valid_epoch_acc /= len(validation_loader.dataset)\n",
    "\n",
    "        # Log validation metrics\n",
    "        validation_loss.append(valid_epoch_loss)\n",
    "        validation_acc.append(valid_epoch_acc)\n",
    "\n",
    "    # Get metrics and prediction probabilities of final model\n",
    "    probabilities = []\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    # Evaluate final model\n",
    "    for batch in validation_loader:\n",
    "        \n",
    "        # Load batch\n",
    "        X = batch[\"Inputs\"]\n",
    "        Y = batch[\"Targets\"]\n",
    "\n",
    "        # Run forward pass\n",
    "        batch_probabilities = model(X)\n",
    "        batch_predictions = batch_probabilities >= CLASS_THR\n",
    "\n",
    "        # Log probabilities, predictions and targets\n",
    "        probabilities.extend(batch_probabilities.detach().numpy().flatten().tolist())\n",
    "        predictions.extend(batch_predictions.detach().numpy().flatten().tolist())\n",
    "        targets.extend(Y.detach().numpy().flatten().tolist())\n",
    "\n",
    "    # Cast to numpy array\n",
    "    probabilities = np.array(probabilities)\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    # Pack all results in a dictionary\n",
    "    training_parameters = {\n",
    "        \"train loss\": training_loss,\n",
    "        \"train acc\": training_acc,\n",
    "        \"validation loss\": validation_loss,\n",
    "        \"validation acc\": validation_acc,\n",
    "        \"Prediction probabilities\": probabilities,\n",
    "        \"Predictions\": predictions,\n",
    "        \"Targets\": targets,\n",
    "    }\n",
    "\n",
    "    return model, training_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treniranje različitih modela\n",
    "\n",
    "U ovom odeljku je definisano tri modela iste arhitekture, ali sa drugačijim hiperparametrima.\n",
    "Za *loss* funkciju izabrana je **binarna krosentropija**, pošto rezultati predstavljaju klasifikaciju igrača u dve klase (opstaće/neće opstati u ligi).\n",
    "\n",
    "Od optimizacionih funkcija, eksperimentisano je sa **stohastičkim gradijentalnim spustom** i **Adamovim optimizatorem**.\n",
    "Adam je pokazao konstantno bolje rezultate, pa je on korišten za sve primere.\n",
    "\n",
    "Svi modeli su trenirani na istom datasetu, istim redosledom batcheva i na **400 epoha**.\n",
    "\n",
    "U nastavku su definisane tri arhitekture: jedna koja je nedovoljno obučena, jedna koja je preobučena, i jedna koja je adekvatna.\n",
    "Slede definicije modela, njihovi treninzi i performanse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# Initialize loss function as Binary Cross Entropy\n",
    "loss_function = BCELoss()\n",
    "\n",
    "# Define learning rate\n",
    "lr = 3e-4\n",
    "\n",
    "# Define number of epochs\n",
    "num_of_epochs = 400\n",
    "\n",
    "# Wrap training loaders into dictionary\n",
    "training_objects = {\n",
    "    \"Loss Function\": loss_function,\n",
    "    \"Train Dataloader\": training_loader,\n",
    "    \"Valid Dataloader\": validation_loader,\n",
    "    \"Epochs\": num_of_epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicija, trening i evaluacija"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treniranje modela sa premalom arhitekturom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with no hidden layers\n",
    "underfit_model = NBA_Survival_Predictor([train_dataset.inputs.shape[1], 1])\n",
    "\n",
    "# Define optimizer for model with 5 times smaller learning rate\n",
    "underfit_optimizer = Adam(params=underfit_model.parameters(), lr=lr/5)\n",
    "\n",
    "# Train and evaluate model\n",
    "underfit_model, underfit_results = train_network(underfit_model, underfit_optimizer, training_objects=training_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treniranje modela sa prevelikom arhitekturom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with 2 hidden layers\n",
    "overfit_model = NBA_Survival_Predictor([train_dataset.inputs.shape[1], 20, 4, 1])\n",
    "\n",
    "# Define optimizer for model with 3 times bigger learning rate\n",
    "overfit_optimizer = Adam(params=overfit_model.parameters(), lr=lr*3)\n",
    "\n",
    "# Train and evaluate model\n",
    "overfit_model, overfit_results = train_network(overfit_model, overfit_optimizer, training_objects=training_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treniranje modela sa adekvatnom arhitekturom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with 1 hidden layer\n",
    "overfit_model = NBA_Survival_Predictor([train_dataset.inputs.shape[1], 16, 1])\n",
    "\n",
    "# Define optimizer for model with original learning rate\n",
    "overfit_optimizer = Adam(params=overfit_model.parameters(), lr=lr)\n",
    "\n",
    "# Train and evaluate model\n",
    "final_model, final_results = train_network(overfit_model, overfit_optimizer, training_objects=training_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prikaz rezultata treninga i validacije\n",
    "\n",
    "Prikazani su rezultati za sva tri modela.\n",
    "U prvom redu su prikazane vrednosti *loss* funkcije po epohama, za validaciju i trening.\n",
    "U drugom redu su prikazane tačnosti modela na treningu i validaciji po epohama.\n",
    "U trećem redu grafika su postavljene konfuzione matrice na validacionom setu na kraju treninga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 3x3 subplots\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(20, 15))\n",
    "\n",
    "# Initialize graph column names\n",
    "title_labels = [\"Underfit\", \"Good fit\", \"Overfit\"]\n",
    "\n",
    "# Loop over each column and generate graphs\n",
    "for idx, results in enumerate([underfit_results, overfit_results, final_results]):\n",
    "\n",
    "    # Plot loss\n",
    "    ax[0, idx].plot(results[\"train loss\"], label=\"Train\")\n",
    "    ax[0, idx].plot(results[\"validation loss\"], label=\"Validation\")\n",
    "    ax[0, idx].legend()\n",
    "    ax[0, idx].set_title(f\"{title_labels[idx]} loss\")\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax[1, idx].plot(results[\"train acc\"], label=\"Train\")\n",
    "    ax[1, idx].plot(results[\"validation acc\"], label=\"Validation\")\n",
    "    ax[1, idx].legend()\n",
    "    ax[1, idx].set_title(f\"{title_labels[idx]} accuracy\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    sb.heatmap(confusion_matrix(results[\"Targets\"], results[\"Predictions\"]), vmin=0, vmax=len(validation_loader.dataset), annot=True, ax=ax[2, idx])\n",
    "    ax[2, idx].set_title(f\"{title_labels[idx]} confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na graficima se može primetiti karakteristično ponašanje za sva tri modela.\n",
    "\n",
    "Model bez skrivenih slojeva nije dovoljno kompleksan da bi efikasno vršio predviđanja.\n",
    "Ovo se oslikava u bajesu modela, koji se primećuje u konstantnoj razlici između tačnosti na treningu i validaciji.\n",
    "\n",
    "Model sa 2 skrivena sloja preobučava.\n",
    "Očigledan simptom preobučavanja su funkcije *loss*-a i tačnosti na validaciji i treningu, koje posle 100. epohe počinju sve više da se udaljavaju jedna od druge.\n",
    "\n",
    "Model sa 1 skrivenim slojem pokazuje najbolje performanse.\n",
    "Funkcije treninga i validacije imaju isti oblik i polako rastu kroz epohe.\n",
    "*Loss* funkcije imaju željenu eksponencijalnu raspodelu.\n",
    "\n",
    "Zbog male veličine modela, trening za sva tri traje slično vreme, oko minut i 30 sekundi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preciznost/Odziv krive najboljeg modela\n",
    "\n",
    "Nakon odabira najboljeg modela, menjan je prag odsecanja i analizirani su preciznost i odziv modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate precision and recall curve points\n",
    "prec, recall, thrs = precision_recall_curve(final_results[\"Targets\"], final_results[\"Prediction probabilities\"])\n",
    "\n",
    "# Calculate f1-score\n",
    "f1 = prec * recall / (prec + recall)\n",
    "\n",
    "# Plot precision/recall/f1 curves\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "plt.plot(thrs, prec[:-1], label=\"Precision\")\n",
    "plt.plot(thrs, recall[:-1], label=\"Recall\")\n",
    "plt.plot(thrs, f1[:-1], label=\"F1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcija preciznosti u rasponu od $0$ do $0.5$ raste sporije nego što funkcija odziva pada u istom segmentu.\n",
    "Kako je cilj modela da predvidi koje igrače treba zadržati u klubu, poželjan je visok odziv.\n",
    "Visok odziv odgovora pošto su igrači u prvim godinama karijere relativno jeftini za zadržavanje, a uvek ih je moguće menjati kasnije.\n",
    "\n",
    "Sa druge strane, preciznost ne sme pasti značajno, kako klub ne bi plaćao igrače koji neće igrati na duže staze.\n",
    "\n",
    "Kompromis između ova dva cilja možemo naći na pragu odsecanja od $0.45$.\\\n",
    "Ovim smanjenjem praga će tačnost pasti, ali če odziv skočiti. Ova promena obezbeđuje da model zadržava više igrača koji će igrati u NBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final threshold\n",
    "final_threshold = 0.45\n",
    "\n",
    "# Get index of threshold with value of final threshold\n",
    "thr_idx = sum(thrs <= final_threshold)\n",
    "\n",
    "# Get best model precision and recall\n",
    "final_precision = prec[thr_idx]\n",
    "final_recall = recall[thr_idx]\n",
    "final_f1 = f1[thr_idx]\n",
    "\n",
    "# Calculate accuracy with final threshold\n",
    "final_accuracy = sum(final_results[\"Targets\"] == (final_results[\"Prediction probabilities\"] > final_threshold))\n",
    "final_accuracy /= len(final_results[\"Targets\"])\n",
    "\n",
    "# Print final results\n",
    "print(\"Statistika finalnog modela:\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"\\tTačnost:    {final_accuracy * 100:.1f}%\")\n",
    "print(f\"\\tPreciznost: {final_precision * 100:.1f}%\")\n",
    "print(f\"\\tOdziv:      {final_recall * 100:.1f}%\")\n",
    "print(f\"\\tF1 skor:    {final_accuracy * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatsko traženje hyperparametara\n",
    "\n",
    "Nakon ručnog odabira hiperparametara, implementirana je automatska pretraga.\n",
    "U ovu svrhu upotrebljena je biblioteka ```optuna```, koju je koristiti sa već implementiranim modelom bez izmena.\n",
    "\n",
    "Biblioteka radi generisanjem *study* sesije, kojoj treba zadati fukciju koju treba optimizovati.\n",
    "Ova funkcija se definiše odvojeno, i u njoj je moguće generisati hiperparametre iz proizvoljne raspodele i nakon toga trenirati tako definisan model.\n",
    "\n",
    "*Study* sesiji se takođe prosleđuje i sampler.\n",
    "Cilj samplera je blago menjanje funkcija raspodela za odabir hiperparametara u zavisnosti od njihove performanse tokom sesije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcija cilja\n",
    "\n",
    "U funkciji cilja su birani *learning rate*, broj skrivenih slojeva, kao i veličina svakog od skrivenih slojeva.\n",
    "*Learning rate* je biran iz raspodele koja je uniformna na logaritamskoj skali u rasponu od $10^{-6}$ do $10^{-4}$.\n",
    "Broj skrivenih slojeva je biran nasumično između 1, 2, 3 i 4, a svaki layer ima između 3 i 20 neurona.\n",
    "\n",
    "Svaki model je treniran na $500$ epoha.\n",
    "\n",
    "Rezultat funkcije cilja je maksimalna tačnost na validaciji modela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"Objective function for NBA_Survival_Predictor training.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "\n",
    "    Returns:\n",
    "        float: Maximal validation accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Pick learning rate from loguniform distribution\n",
    "    lr = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4)\n",
    "    \n",
    "    # Pick number of layers\n",
    "    num_of_layers = trial.suggest_int(\"num_of_layers\", 1, 4)\n",
    "\n",
    "    # Initialize layer size list with input layer size\n",
    "    layer_sizes = [train_dataset.inputs.shape[1]]\n",
    "\n",
    "    # For each layer pick its size\n",
    "    for idx in range(num_of_layers):\n",
    "        layer_sizes.append(trial.suggest_int(f\"layer_{idx}\", 3, 20))\n",
    "    \n",
    "    # Add output layer size to layer sizes list\n",
    "    layer_sizes.append(1)\n",
    "\n",
    "    # Init model\n",
    "    model = NBA_Survival_Predictor(layer_sizes)\n",
    "    \n",
    "    # Init optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Set number of epochs\n",
    "    training_objects[\"Epochs\"] = 500\n",
    "\n",
    "    # Train and evaluate model\n",
    "    _, results = train_network(model, optimizer, training_objects)\n",
    "\n",
    "    # Return best performance\n",
    "    return max(results[\"validation acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traženje najboljih hiperparametara\n",
    "\n",
    "U nastavku je kreiran *study* koji pokušava da maksimizuje prethodnu funkciju cilja.\n",
    "Definisan je maksimum od $100$ pokušaja za maksimizaciju funkcije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective_function, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redukcija dimenzionalnosti\n",
    "\n",
    "Kao poslednji pokušaj za unapređenje modela, implementirana je redukcija dimenzionalnosti *dataset*-a.\\\n",
    "Implementirana je **LDA** redukcija i set atributa je redukovan na $4$.\\\n",
    "Redukcija je implementirana na normalizovanim podacima.\n",
    "\n",
    "Nad ovakvim podacima treniran je perceptron, zbog malog broja atributa.\n",
    "*Learning rate* je menjan u ```Optuna``` *study*-u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA redukcija\n",
    "Kod ispod generiše kovariacionu matricu atributa i računa njihove sopstvene vektore.\\\n",
    "Od $4$ vektora, koji odgovaraju atributima sa najvećim sopstvenim vrednostim, se formira matrica kojom ce biti transformisan *dataset*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance matrix of attributes, without targets\n",
    "cov_mat = norm_train_df.corr().to_numpy()[:-1, :-1]\n",
    "\n",
    "# Calculate eigenvalues\n",
    "eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "\n",
    "# Take vector subset\n",
    "num_of_components = 4\n",
    "eigv_subset = eigen_vectors[:, -num_of_components:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformišemo trening i test podatke dobijenom matricom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numpy matrix of training attributes\n",
    "X = norm_train_df.to_numpy()[:, :-1]\n",
    "\n",
    "# Calculate reduced set of attributes\n",
    "X_reduced = np.dot(eigv_subset.T, X.T).T\n",
    "\n",
    "# Initialize datafreme with reduced attributes\n",
    "reduced_train_df = pd.DataFrame(X_reduced, columns=[f\"feature {idx}\" for idx in range(num_of_components)])\n",
    "\n",
    "# Add target values to the dataframe\n",
    "reduced_train_df[\"target\"] = norm_train_df[\"target\"]\n",
    "\n",
    "# Print results\n",
    "print(\"Redukovani trening dataframe:\")\n",
    "reduced_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numpy matrix of validation attributes\n",
    "X = norm_valid_df.to_numpy()[:, :-1]\n",
    "\n",
    "# Calculate reduced set of attributes\n",
    "X_reduced = np.dot(eigv_subset.T, X.T).T\n",
    "\n",
    "# Initialize datafreme with reduced attributes\n",
    "reduced_valid_df = pd.DataFrame(X_reduced, columns=[f\"feature {idx}\" for idx in range(num_of_components)])\n",
    "\n",
    "# Add target values to the dataframe\n",
    "reduced_valid_df[\"target\"] = norm_valid_df[\"target\"]\n",
    "\n",
    "# Print results\n",
    "print(\"Redukovani validacioni dataframe:\")\n",
    "reduced_valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generisanje *dataloader*-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_train_dataset = NBA_Dataset(reduced_train_df)\n",
    "\n",
    "train_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "\n",
    "reduced_training_loader = DataLoader(reduced_train_dataset, **train_parameters)\n",
    "\n",
    "# Validation dataset\n",
    "reduced_valid_dataset = NBA_Dataset(reduced_valid_df)\n",
    "\n",
    "valid_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 1,\n",
    "}\n",
    "\n",
    "reduced_validation_loader = DataLoader(reduced_valid_dataset, **valid_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisanje trening objekata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training objects dictionary\n",
    "training_objects = {\n",
    "    \"Loss Function\": loss_function,\n",
    "    \"Train Dataloader\": reduced_training_loader,\n",
    "    \"Valid Dataloader\": reduced_validation_loader,\n",
    "    \"Epochs\": num_of_epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treniranje modela\n",
    "\n",
    "Pošto je trenirani model perceptron, jedini hiperparametar koji poseduje je *learning rate*.\n",
    "Implementirana je nova funkcija cilja koja pretražuje optimalan *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_reduced(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"Objective function for NBA_Survival_Predictor training, with reduced feature space.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Optuna trial object.\n",
    "\n",
    "    Returns:\n",
    "        float: Maximal validation accuracy of the model.\n",
    "    \"\"\"\n",
    "    # Pick learning rate from loguniform distribution\n",
    "    lr = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4)\n",
    "\n",
    "    layer_sizes = [4, 1]\n",
    "\n",
    "    # Init model\n",
    "    model = NBA_Survival_Predictor(layer_sizes)\n",
    "    \n",
    "    # Init optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Set number of epochs\n",
    "    training_objects[\"Epochs\"] = 300\n",
    "\n",
    "    # Train and evaluate model\n",
    "    _, results = train_network(model, optimizer, training_objects)\n",
    "\n",
    "    # Return best performance\n",
    "    return max(results[\"validation acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective_function_reduced, n_trials=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Damjan Denic\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "<ipython-input-114-ea512f31a27a>:15: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  total_ball_data = total_ball_data.fillna(total_ball_data.mean())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Path of file to read\n",
    "data_path = 'inputs/CollegeBasketballPlayers2009-2021.csv'\n",
    "total_ball_data = pd.read_csv(data_path, low_memory = False)\n",
    "total_ball_data['pick'] = total_ball_data['pick'].fillna(0)\n",
    "total_ball_data['pick'].loc[(total_ball_data['pick'] > 1)] = 1\n",
    "#one-hot encoding\n",
    "total_ball_data = pd.concat([total_ball_data,pd.get_dummies(total_ball_data['yr'],prefix='yr')],axis=1).drop(['yr'],axis=1)\n",
    "total_ball_data = total_ball_data.drop(['yr_0', 'yr_42.9','yr_57.1', 'yr_None'],axis = 1)\n",
    "#print(total_ball_data.columns)\n",
    "\n",
    "# fill NaNs with means\n",
    "total_ball_data = total_ball_data.fillna(total_ball_data.mean())\n",
    "\n",
    "# Player names\n",
    "\n",
    "player_names = total_ball_data[['player_name', 'team']]\n",
    "\n",
    "# Only include rows in which a player was drafted, and before 2021 (2021 will be used as test data)\n",
    "\n",
    "ball_data_train = total_ball_data.loc[total_ball_data['year'] < 2021]\n",
    "ball_data_train_truth = ball_data_train['pick']\n",
    "ball_data_train = ball_data_train[['GP', 'Min_per', 'Ortg', 'usg', 'eFG',\\\n",
    "       'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM', 'FTA',\\\n",
    "       'FT_per', 'twoPM', 'twoPA', 'twoP_per', 'TPM', 'TPA', 'TP_per',\\\n",
    "       'blk_per', 'stl_per', 'ftr', 'porpag', 'adjoe', 'pfr','ast/tov',\\\n",
    "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm', 'mp',\\\n",
    "       'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk', 'pts', \\\n",
    "       'yr_Sr', 'yr_So', 'yr_Fr', 'yr_Jr']]\n",
    "\n",
    "ball_data_test = total_ball_data.loc[total_ball_data['year'] == 2021]\n",
    "ball_data_test_truth = ball_data_test['pick']\n",
    "ball_data_test = ball_data_test[['GP', 'Min_per', 'Ortg', 'usg', 'eFG',\\\n",
    "       'TS_per', 'ORB_per', 'DRB_per', 'AST_per', 'TO_per', 'FTM', 'FTA',\\\n",
    "       'FT_per', 'twoPM', 'twoPA', 'twoP_per', 'TPM', 'TPA', 'TP_per',\\\n",
    "       'blk_per', 'stl_per', 'ftr', 'porpag', 'adjoe', 'pfr','ast/tov',\\\n",
    "       'drtg', 'adrtg', 'dporpag', 'stops', 'bpm', 'obpm', 'dbpm', 'gbpm', 'mp',\\\n",
    "       'ogbpm', 'dgbpm', 'oreb', 'dreb', 'treb', 'ast', 'stl', 'blk', 'pts', \\\n",
    "       'yr_Sr', 'yr_So', 'yr_Fr', 'yr_Jr']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NbaDataset(T.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, ds, ds_truth):\n",
    "    self.x_data = T.tensor(ball_data_train.values.astype(np.float32),\n",
    "        dtype=T.float32).to(device)\n",
    "    self.y_data = T.tensor(ball_data_train_truth.values.astype(np.float32),\n",
    "        dtype=T.float32).to(device)\n",
    "    self.y_data = self.y_data.reshape(-1,1)\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if T.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "    preds = self.x_data[idx,:]  # idx rows, all 4 cols\n",
    "    lbl = self.y_data[idx,:]    # idx rows, the 1 col\n",
    "    sample = { 'predictors' : preds, 'target' : lbl }\n",
    "    # sample = dict()   # or sample = {}\n",
    "    # sample[\"predictors\"] = preds\n",
    "    # sample[\"target\"] = lbl\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-c472d9208749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# apply to Tensor or Module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNbaDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mball_data_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mball_data_train_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNbaDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mball_data_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mball_data_test_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-4515518ca2d4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ds, ds_truth)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     self.x_data = T.tensor(ball_data_train.values.astype(np.float32),\n\u001b[0m\u001b[0;32m      5\u001b[0m         dtype=T.float32).to(device)\n\u001b[0;32m      6\u001b[0m     self.y_data = T.tensor(ball_data_train_truth.values.astype(np.float32),\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "device = T.device(\"cuda\")  # apply to Tensor or Module\n",
    "\n",
    "train_ds = NbaDataset(ball_data_train,ball_data_train_truth)\n",
    "test_ds = NbaDataset(ball_data_test,ball_data_test_truth)\n",
    "\n",
    "bat_size = 10\n",
    "train_ldr = T.utils.data.DataLoader(train_ds,\n",
    "batch_size=bat_size, shuffle=False)\n",
    "\n",
    "# 2. create neural network\n",
    "print(\"Creating binary NN classifier \")\n",
    "net = Net().to(device)\n",
    "\n",
    "# 3. train network\n",
    "print(\"\\nPreparing training\")\n",
    "net = net.train()  # set training mode\n",
    "lrn_rate = 0.01\n",
    "loss_obj = T.nn.BCELoss()  # binary cross entropy\n",
    "optimizer = T.optim.SGD(net.parameters(),\n",
    "lr=lrn_rate)\n",
    "max_epochs = 100\n",
    "ep_log_interval = 10\n",
    "print(\"Loss function: \" + str(loss_obj))\n",
    "print(\"Optimizer: SGD\")\n",
    "print(\"Learn rate: 0.01\")\n",
    "print(\"Batch size: 10\")\n",
    "print(\"Max epochs: \" + str(max_epochs))\n",
    "\n",
    "print(\"\\nStarting training\")\n",
    "for epoch in range(0, max_epochs):\n",
    "    epoch_loss = 0.0            # for one full epoch\n",
    "    epoch_loss_custom = 0.0\n",
    "    num_lines_read = 0\n",
    "\n",
    "    for (batch_idx, batch) in enumerate(train_ldr):\n",
    "        X = batch['predictors']  # [10,4]  inputs\n",
    "        Y = batch['target']      # [10,1]  targets\n",
    "        oupt = net(X)            # [10,1]  computeds \n",
    "\n",
    "        loss_val = loss_obj(oupt, Y)   # a tensor\n",
    "        epoch_loss += loss_val.item()  # accumulate\n",
    "        # epoch_loss += loss_val  # is OK\n",
    "        # epoch_loss_custom += my_bce(net, batch)\n",
    "\n",
    "        optimizer.zero_grad() # reset all gradients\n",
    "        loss_val.backward()   # compute all gradients\n",
    "        optimizer.step()      # update all weights\n",
    "\n",
    "    if epoch % ep_log_interval == 0:\n",
    "        print(\"epoch = %4d   loss = %0.4f\" % \\\n",
    "        (epoch, epoch_loss))\n",
    "        # print(\"custom loss = %0.4f\" % epoch_loss_custom)\n",
    "        # print(\"\")\n",
    "print(\"Done \")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# 4. evaluate model\n",
    "net = net.eval()\n",
    "acc_train = accuracy(net, train_ds)\n",
    "print(\"\\nAccuracy on train data = %0.2f%%\" % \\\n",
    "(acc_train * 100))\n",
    "acc_test = accuracy(net, test_ds)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "(acc_test * 100))\n",
    "\n",
    "# acc_train_c = acc_coarse(net, train_ds)\n",
    "# print(\"Accuracy on train data = %0.2f%%\" % \\\n",
    "#  (acc_train_c * 100))\n",
    "# acc_test_c = acc_coarse(net, test_ds)\n",
    "# print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "#  (acc_test_c * 100))\n",
    "\n",
    "# 5. save model\n",
    "print(\"\\nSaving trained model state_dict \\n\")\n",
    "path = \"Models\\\\banknote_sd_model.pth\"\n",
    "T.save(net.state_dict(), path)\n",
    "\n",
    "# print(\"\\nSaving entire model \\n\")\n",
    "# path = \".\\\\Models\\\\banknote_full_model.pth\"\n",
    "# T.save(net, path\n",
    "\n",
    "# print(\"\\nSaving trained model as ONNX \\n\")\n",
    "# path = \".\\\\Models\\\\banknote_onnx_model.onnx\"\n",
    "# dummy = T.tensor([[0.5, 0.5, 0.5, 0.5]],\n",
    "#   dtype=T.float32).to(device)\n",
    "# T.onnx.export(net, dummy, path,\n",
    "#   input_names=[\"input1\"],\n",
    "#  output_names=[\"output1\"])\n",
    "\n",
    "# model = Net()  # later . . \n",
    "# model.load_state_dict(T.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, ds):\n",
    "  # ds is a iterable Dataset of Tensors\n",
    "  n_correct = 0; n_wrong = 0\n",
    "\n",
    "  # alt: create DataLoader and then enumerate it\n",
    "  for i in range(len(ds)):\n",
    "    inpts = ds[i]['predictors']\n",
    "    target = ds[i]['target']    # float32  [0.0] or [1.0]\n",
    "    with T.no_grad():\n",
    "      oupt = model(inpts)\n",
    "\n",
    "    # avoid 'target == 1.0'\n",
    "    if target < 0.5 and oupt < 0.5:  # .item() not needed\n",
    "      n_correct += 1\n",
    "    elif target >= 0.5 and oupt >= 0.5:\n",
    "      n_correct += 1\n",
    "    else:\n",
    "      n_wrong += 1\n",
    "\n",
    "  return (n_correct * 1.0) / (n_correct + n_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_coarse(model, ds):\n",
    "  inpts = ds[:]['predictors']  # all rows\n",
    "  targets = ds[:]['target']    # all target 0s and 1s\n",
    "  with T.no_grad():\n",
    "    oupts = model(inpts)         # all computed ouputs\n",
    "  pred_y = oupts >= 0.5        # tensor of 0s and 1s\n",
    "  num_correct = T.sum(targets==pred_y)\n",
    "  acc = (num_correct.item() * 1.0 / len(ds))  # scalar\n",
    "  return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bce(model, batch):\n",
    "  # mean binary cross entropy error. somewhat slow\n",
    "  sum = 0.0\n",
    "  inputs = batch['predictors']\n",
    "  targets = batch['target']\n",
    "  with T.no_grad():\n",
    "    oupts = model(inputs)\n",
    "  for i in range(len(inputs)):\n",
    "    oupt = oupts[i]\n",
    "    # should prevent log(0) which is -infinity\n",
    "    if targets[i] >= 0.5:  # avoiding == 1.0\n",
    "      sum += T.log(oupt)\n",
    "    else:\n",
    "      sum += T.log(1 - oupt)\n",
    "\n",
    "  return -sum / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.hid1 = T.nn.Linear(48, 96)  # 4-(8-8)-1\n",
    "    self.hid2 = T.nn.Linear(96, 48)\n",
    "    self.hid3 = T.nn.Linear(48, 24)\n",
    "    self.hid4 = T.nn.Linear(24, 8)\n",
    "    self.oupt = T.nn.Linear(8, 1)\n",
    "\n",
    "    T.nn.init.xavier_uniform_(self.hid1.weight) \n",
    "    T.nn.init.zeros_(self.hid1.bias)\n",
    "    T.nn.init.xavier_uniform_(self.hid2.weight) \n",
    "    T.nn.init.zeros_(self.hid2.bias)\n",
    "    T.nn.init.xavier_uniform_(self.hid3.weight) \n",
    "    T.nn.init.zeros_(self.hid3.bias)\n",
    "    T.nn.init.xavier_uniform_(self.hid4.weight) \n",
    "    T.nn.init.zeros_(self.hid4.bias)\n",
    "    T.nn.init.xavier_uniform_(self.oupt.weight) \n",
    "    T.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "  def forward(self, x):\n",
    "    z = T.tanh(self.hid1(x)) \n",
    "    z = T.tanh(self.hid2(z))\n",
    "    z = T.tanh(self.hid3(z))\n",
    "    z = T.tanh(self.hid4(z))\n",
    "    z = T.sigmoid(self.oupt(z)) \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "# Creating np arrays\n",
    "target = df['Target'].values\n",
    "features = df.drop('Target', axis=1).values\n",
    "\n",
    "# Passing to DataLoader\n",
    "train = data_utils.TensorDataset(features, target)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-f3812d208860>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\Damjan Denic\\\\OneDrive\\\\Documents\\\\ETF\\\\NM\\\\nba\\\\Models\\\\banknote_sd_model.pth\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# later . .\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 4. evaluate model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "device = T.device(\"cuda\")  # apply to Tensor or Module\n",
    "path = \"C:\\\\Users\\\\Damjan Denic\\\\OneDrive\\\\Documents\\\\ETF\\\\NM\\\\nba\\\\Models\\\\banknote_sd_model.pth\"\n",
    "net = Net()  # later . . \n",
    "net.load_state_dict(T.load(path))\n",
    "\n",
    "# 4. evaluate model\n",
    "net = net.eval()\n",
    "acc_train = accuracy(net, train_ds)\n",
    "print(\"\\nAccuracy on train data = %0.2f%%\" % \\\n",
    "(acc_train * 100))\n",
    "acc_test = accuracy(net, test_ds)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "(acc_test * 100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77e579291131ae26057fcf5613a2366167539e8d1df4ead859a5a3341fa42dc0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
